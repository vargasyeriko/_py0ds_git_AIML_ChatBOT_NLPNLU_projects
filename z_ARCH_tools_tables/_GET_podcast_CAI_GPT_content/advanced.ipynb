{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403abd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to research_topics_combined.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First chunk\n",
    "first_chunk = {\n",
    "    'Category': ['Music Recommendation', 'Audio Signal Processing', 'Podcast Analytics', 'Speech Recognition', 'Sound Localization'],\n",
    "    'Research Topic / Statement': [\n",
    "        'Develop a recommendation algorithm for playlist curation based on audio features.',\n",
    "        'Explore noise cancellation techniques using machine learning algorithms.',\n",
    "        'Analyzing listener behavior and preferences through podcast audio and metadata.',\n",
    "        'Using neural networks for real-time voice-to-text conversion.',\n",
    "        'Implementing algorithms for sound source localization in a 3D space.'\n",
    "    ],\n",
    "    'Instagram Caption with Emoji': [\n",
    "        'Revolutionize Your Playlist üéµü§ñ',\n",
    "        'Tune Out The Noise üéßüôå',\n",
    "        'Unlock Your Podcast\\'s Potential üéôÔ∏èüîç',\n",
    "        'Talk to Text, Instantly üó£Ô∏èüìù',\n",
    "        'Find Where Sound Originates üåêüëÇ'\n",
    "    ],\n",
    "    'Instagram Hashtags': [\n",
    "        '#TuneTech, #AIPlaylistWizard',\n",
    "        '#QuietGenius, #MLQuietZone',\n",
    "        '#PodcastNerds, #DeepDiveData',\n",
    "        '#SpeakAndSeek, #VocalBytes',\n",
    "        '#3DSoundHunt, #AudioMapping'\n",
    "    ],\n",
    "    'LinkedIn Hashtags': [\n",
    "        '#MusicTech, #DataScience',\n",
    "        '#SignalProcessing, #AI',\n",
    "        '#PodcastData, #Analytics',\n",
    "        '#SpeechRecognition, #NaturalLanguageProcessing',\n",
    "        '#SpatialAudio, #MachineLearning'\n",
    "    ],\n",
    "    'Twitter Hashtags': [\n",
    "        '#PlaylistRevolution, #MusicAndAI',\n",
    "        '#NoiseFreeZone, #SilentScience',\n",
    "        '#DataDrivenPodcast, #PodcastStats',\n",
    "        '#TextByVoice, #Vocal2Text',\n",
    "        '#WhereIsThatSound, #AudioCoordinates'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Second chunk\n",
    "second_chunk = {\n",
    "    'Category': ['Audio Classification', 'Sentiment Analysis', 'Sound Synthesis', 'Acoustic Scene Analysis', 'Audio Watermarking'],\n",
    "    'Research Topic / Statement': [\n",
    "        'Classifying musical genres using machine learning.',\n",
    "        'Emotional tone detection in spoken language within customer service calls.',\n",
    "        'Using GANs for the generation of musical notes or sound effects.',\n",
    "        'Identifying environmental context through sound.',\n",
    "        'Develop secure and robust audio watermarking techniques.'\n",
    "    ],\n",
    "    'Instagram Caption with Emoji': [\n",
    "        'Identify Any Genre üé∂üè∑Ô∏è',\n",
    "        'Feel the Customer\\'s Pulse üìûüíñ',\n",
    "        'Create Sound with AI üéµü§ñ',\n",
    "        'Hear the Environment üå≥üëÇ',\n",
    "        'Secure Your Sound üîíüé∂'\n",
    "    ],\n",
    "    'Instagram Hashtags': [\n",
    "        '#GenreGenius, #TuneTag',\n",
    "        '#EmoToneDetect, #CustomerFeel',\n",
    "        '#AISoundMaker, #GANBeats',\n",
    "        '#EnviroListen, #SceneSound',\n",
    "        '#AudioLock, #SoundSecurity'\n",
    "    ],\n",
    "    'LinkedIn Hashtags': [\n",
    "        '#AudioClassification, #ML',\n",
    "        '#SentimentAnalysis, #CustomerService',\n",
    "        '#SoundSynthesis, #GAN',\n",
    "        '#AcousticScene, #EnvironmentalSound',\n",
    "        '#AudioWatermarking, #Security'\n",
    "    ],\n",
    "    'Twitter Hashtags': [\n",
    "        '#GenreWizard, #MLMusic',\n",
    "        '#MoodMeter, #CSAnalysis',\n",
    "        '#SoundByAI, #GANNotes',\n",
    "        '#SceneSensing, #NatureSound',\n",
    "        '#WatermarkWiz, #SecureSound'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert dictionaries to DataFrames\n",
    "df1 = pd.DataFrame(first_chunk)\n",
    "df2 = pd.DataFrame(second_chunk)\n",
    "\n",
    "# Concatenate both DataFrames vertically\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Write combined DataFrame to Excel\n",
    "excel_path = \"research_topics_combined.xlsx\"  # ^^^ path ^^^\n",
    "df_combined.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been written to {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee8ba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to research_topics_extended.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Third chunk\n",
    "third_chunk = {\n",
    "    'Category': ['Audio Compression', 'Sound Equalization', 'Automated Mixing', 'Real-Time Audio Effects', 'ML for Mastering'],\n",
    "    'Research Topic / Statement': [\n",
    "        'Optimizing audio file sizes without quality loss through AI-driven compression algorithms.',\n",
    "        'Adaptive sound equalization for optimal listening experiences across devices.',\n",
    "        'Automating audio mixing tasks using machine learning techniques.',\n",
    "        'Implementing real-time audio effects through AI for live performances.',\n",
    "        'Utilizing machine learning algorithms for mastering audio tracks.'\n",
    "    ],\n",
    "    'Instagram Caption with Emoji': [\n",
    "        'Compress without Compromise üé∂üíæ',\n",
    "        'Tailored Sound Experience üéµüîä',\n",
    "        'Smart Mixing Desk üéöÔ∏èü§ñ',\n",
    "        'Real-time Audio Magic üé§‚ú®',\n",
    "        'Master Tracks Like a Pro üéßüåü'\n",
    "    ],\n",
    "    'Instagram Hashtags': [\n",
    "        '#SmartCompression, #AICompression',\n",
    "        '#SoundTailoring, #AdaptiveEqualization',\n",
    "        '#AutomatedMix, #MLMixing',\n",
    "        '#LiveFX, #AIRealTime',\n",
    "        '#MLMastering, #ProAudio'\n",
    "    ],\n",
    "    'LinkedIn Hashtags': [\n",
    "        '#AudioCompression, #AI',\n",
    "        '#SoundEqualization, #AdaptiveAudio',\n",
    "        '#AutomatedMixing, #MachineLearning',\n",
    "        '#RealTimeEffects, #LiveAudio',\n",
    "        '#Mastering, #AudioQuality'\n",
    "    ],\n",
    "    'Twitter Hashtags': [\n",
    "        '#CompressAndImpress, #QualityAndQuantity',\n",
    "        '#EqualizeMe, #DeviceSound',\n",
    "        '#SmartMix, #TechAndTunes',\n",
    "        '#RealTimeWonders, #AIinLive',\n",
    "        '#MasterWithML, #SoundScience'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert third_chunk to DataFrame\n",
    "df3 = pd.DataFrame(third_chunk)\n",
    "\n",
    "# Concatenate all DataFrames vertically\n",
    "df_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Write combined DataFrame to Excel\n",
    "excel_path = \"research_topics_extended.xlsx\"  # ^^^ path ^^^\n",
    "df_combined.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been written to {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3c72f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to research_topics_full.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Fourth chunk\n",
    "fourth_chunk = {\n",
    "    'Category': ['Beat Detection', 'Audio Segmentation', 'Dynamic Range Control', 'Acoustic Modeling', 'Audio-Based Emotion Recognition'],\n",
    "    'Research Topic / Statement': [\n",
    "        'Creating robust algorithms for automatic beat detection in complex musical compositions.',\n",
    "        'Segmenting audio streams into distinct elements for easier analysis and manipulation.',\n",
    "        'Developing intelligent algorithms for real-time dynamic range control in live sound applications.',\n",
    "        'Modeling acoustic environments to predict sound behavior and reverberation.',\n",
    "        'Recognizing emotional states from voice recordings using machine learning.'\n",
    "    ],\n",
    "    'Instagram Caption with Emoji': [\n",
    "        'Catch the Beat üéµüíó',\n",
    "        'Audio in Pieces üé∂üß©',\n",
    "        'Control the Dynamics üéöÔ∏èüåà',\n",
    "        'Model the Sound üè¢üé∂',\n",
    "        'Voice Reveals Feelings üó£Ô∏èüíñ'\n",
    "    ],\n",
    "    'Instagram Hashtags': [\n",
    "        '#BeatMaster, #RhythmicAI',\n",
    "        '#SegmentGenius, #AudioPuzzle',\n",
    "        '#DynamicControl, #LiveBalance',\n",
    "        '#AcousticModeling, #SoundSpaces',\n",
    "        '#VoiceEmotion, #FeelTheSound'\n",
    "    ],\n",
    "    'LinkedIn Hashtags': [\n",
    "        '#BeatDetection, #MusicTech',\n",
    "        '#AudioSegmentation, #AudioAnalysis',\n",
    "        '#DynamicRange, #SoundControl',\n",
    "        '#AcousticModel, #SoundEnvironment',\n",
    "        '#EmotionRecognition, #VoiceAnalysis'\n",
    "    ],\n",
    "    'Twitter Hashtags': [\n",
    "        '#BeatDetect, #MusicAI',\n",
    "        '#SliceTheSound, #AudioSegments',\n",
    "        '#DynamicAI, #VolumeControl',\n",
    "        '#ModelMySound, #SoundPrediction',\n",
    "        '#VoiceFeelings, #EmotionAI'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert fourth_chunk to DataFrame\n",
    "df4 = pd.DataFrame(fourth_chunk)\n",
    "\n",
    "# Concatenate all DataFrames vertically\n",
    "df_combined = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Write combined DataFrame to Excel\n",
    "excel_path = \"research_topics_full.xlsx\"  # ^^^ path ^^^\n",
    "df_combined.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been written to {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10efb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to research_topics_complete.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Fifth chunk\n",
    "fifth_chunk = {\n",
    "    'Category': ['Audio Synthesis', 'Sound Localization', 'Virtual Reality Soundscapes', 'Audio Fingerprinting', 'Data-Driven Sound Design'],\n",
    "    'Research Topic / Statement': [\n",
    "        'Developing novel methods for audio synthesis using Generative Adversarial Networks (GANs).',\n",
    "        'Accurate sound localization techniques for smart home devices and applications.',\n",
    "        'Creating immersive audio environments for virtual reality experiences.',\n",
    "        'Designing algorithms for audio fingerprinting and matching in large databases.',\n",
    "        'Applying data science techniques to design sound elements in interactive media.'\n",
    "    ],\n",
    "    'Instagram Caption with Emoji': [\n",
    "        'Craft Your Own Sounds üéµüé®',\n",
    "        'Where‚Äôs That Sound Coming From? üé∂üìç',\n",
    "        'Step Into a New World of Sound üéßüåç',\n",
    "        'Find That Tune! üéµüîç',\n",
    "        'Data-Driven Design üìäüé∂'\n",
    "    ],\n",
    "    'Instagram Hashtags': [\n",
    "        '#GANaudio, #SynthesizeMe',\n",
    "        '#SoundLocalize, #SmartHomeSounds',\n",
    "        '#VRaudio, #ImmersiveSound',\n",
    "        '#AudioID, #FindThatTune',\n",
    "        '#SoundData, #DesignWithData'\n",
    "    ],\n",
    "    'LinkedIn Hashtags': [\n",
    "        '#AudioSynthesis, #GANs',\n",
    "        '#SoundLocalization, #SmartHome',\n",
    "        '#VRSound, #AudioEnvironment',\n",
    "        '#AudioFingerprinting, #SearchAlgorithms',\n",
    "        '#DataDrivenDesign, #InteractiveSound'\n",
    "    ],\n",
    "    'Twitter Hashtags': [\n",
    "        '#CraftedAudio, #GANs',\n",
    "        '#LocateSound, #IoT',\n",
    "        '#VRsoundscape, #360Audio',\n",
    "        '#SoundMatch, #AudioSearch',\n",
    "        '#DataSound, #SoundScience'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert fifth_chunk to DataFrame\n",
    "df5 = pd.DataFrame(fifth_chunk)\n",
    "\n",
    "# Concatenate all DataFrames vertically\n",
    "df_combined = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "\n",
    "# Write combined DataFrame to Excel\n",
    "excel_path = \"research_topics_complete.xlsx\"  # ^^^ path ^^^\n",
    "df_combined.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been written to {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caff7ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to research_topics_final.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Sixth chunk\n",
    "sixth_chunk = {\n",
    "    'Category': ['Speech-to-Text Algorithms', 'Audio Signal Restoration', 'Ambient Sound Analysis', 'Machine Listening', 'Sound-Based User Authentication'],\n",
    "    'Research Topic / Statement': [\n",
    "        'Enhancing the accuracy and efficiency of speech-to-text conversion algorithms.',\n",
    "        'Restoring and enhancing aged or damaged audio recordings using machine learning.',\n",
    "        'Analyzing ambient sounds to detect anomalies or specific events.',\n",
    "        'Implementing machine listening to understand and respond to auditory scenes.',\n",
    "        'Developing secure, sound-based user authentication systems.'\n",
    "    ],\n",
    "    'Instagram Caption with Emoji': [\n",
    "        'Speak and Be Heard üó£Ô∏èüìù',\n",
    "        'Restore the Lost Notes üéµüõ†Ô∏è',\n",
    "        'Sounds of the Environment üå≥üîä',\n",
    "        'Machines That Listen üëÇü§ñ',\n",
    "        'Unlock with Sound üîíüé∂'\n",
    "    ],\n",
    "    'Instagram Hashtags': [\n",
    "        '#SpeechToText, #VoiceTech',\n",
    "        '#AudioRestoration, #SoundRevival',\n",
    "        '#AmbientAnalysis, #EcoSound',\n",
    "        '#MachineListening, #AIHears',\n",
    "        '#SoundAuth, #SecureNotes'\n",
    "    ],\n",
    "    'LinkedIn Hashtags': [\n",
    "        '#SpeechRecognition, #NaturalLanguageProcessing',\n",
    "        '#SignalRestoration, #AudioEngineering',\n",
    "        '#AmbientSound, #EventDetection',\n",
    "        '#AuditoryScene, #AI',\n",
    "        '#UserAuthentication, #Security'\n",
    "    ],\n",
    "    'Twitter Hashtags': [\n",
    "        '#SpeakToType, #AI',\n",
    "        '#RestoreThePast, #AudioFix',\n",
    "        '#EcoSound, #SoundSensing',\n",
    "        '#AIListen, #AuditoryAI',\n",
    "        '#UnlockWithSound, #SoundSecurity'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert sixth_chunk to DataFrame\n",
    "df6 = pd.DataFrame(sixth_chunk)\n",
    "\n",
    "# Concatenate all DataFrames vertically\n",
    "df_combined = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "\n",
    "# Write combined DataFrame to Excel\n",
    "excel_path = \"research_topics_final.xlsx\"  # ^^^ path ^^^\n",
    "df_combined.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Data has been written to {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f386861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
