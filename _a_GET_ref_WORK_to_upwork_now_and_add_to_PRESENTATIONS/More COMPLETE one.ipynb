{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a50340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################################################################### - HI\n",
      "######################################################################################################\n",
      "----------------------------------------------------------------------------------------------------->\n",
      "- PY ---------------------------------------------------------------------------- GLOBAL/_1_paths.py >\n",
      "- MOST COMMON :: <desk_path>:<docs_path>:<down_path>::----------------------------------------------->\n",
      "----------------------------------------------------------------------------------------------------->\n",
      "********************************************  > Directory *\n",
      "\t\t\t\t\t\t\t\t\t -*- [ .ipynb_checkpoints ]\n",
      "\t\t\t\t\t\t\t\t\t -*- [ JUPY_PRESENTATIONS ]\n",
      "\t\t\t\t\t\t\t\t\t -*- [ functions ]\n",
      "\t\t\t\t\t\t\t\t\t -*- [ z_ARCH_upwork_applications ]\n",
      "\n",
      "********************************************  > .ipynb *\n",
      "\t\t\t\t\t\t\t\t\t -*- [ Content.ipynb ]\n",
      "\t\t\t\t\t\t\t\t\t -*- [ RUN.ipynb ]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------->\n",
      "######################################################################################################\n",
      "######################################################################################################\n",
      "*** ATTN * curr_direc = /Users/yerik/_apple_source/PY/JUPY/py0ds/CONTENT *\n",
      "----------------------------------------------------------------------------------------------------->\n",
      "READ temp ::: exec(open(f\"{direc}/_{py_lib}_topic_sub_#_spec_part.py\",encoding=\"utf-8\").read()) \n",
      "WRITE temp ::: %%writefile _0ds_topic_#_spec_part.py                             \n",
      "----------------------------------------------------------------------------------------------------->\n",
      "\n",
      "-1) py RUN ::: \n",
      "\n",
      "%%writefile RUN.py \n",
      "exec(open(f\"{direc}/*latest*.py\", encoding=\"utf-8\").read()\n",
      "\n",
      "-2) py LIB ::: \n",
      "\n",
      "%%writefile /Users/yerik/_apple_source/PY/libs/_0ds/_0ds_*descriptive_program_name*.py \n",
      "direc = \"/Users/yerik/_apple_source/PY/JUPY/py0ds/CONTENT\"\n",
      "exec(open(f\"{direc}/RUN.py\", encoding=\"utf-8\").read()# This Program will be added to ALL_LIBS to execute::\n",
      "----------------------------------------------------------------------------------------------------->\n",
      "######################################################################################################\n",
      "###################################################################################################### - BYE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023_09_14'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import sys;import io ; original_stdout = sys.stdout ; sys.stdout = io.StringIO()          # Suppress this output\n",
    "exec(open(f\"/Users/yerik/_apple_source/PY/GLOBAL/_1_paths.py\", encoding=\"utf-8\").read())  # GLOBAL\n",
    "direc = curr_direc ;# sys.stdout = original_stdout                                         # Reset Suppress\n",
    "#packages\n",
    "import pyperclip\n",
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time\n",
    "now = datetime.now(); date_string = now.strftime(\"%Y_%m_%d\")#date_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8cc646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "exec(open(f\"{direc}/functions/_0ds_fn_1_get_ipynb_file_names.py\", encoding=\"utf-8\").read()) \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef535a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OCR_Based_Data_Management_Virtual_Assistant.ipynb',\n",
       " 'generate_sentences_to_text_Presentation.ipynb',\n",
       " 'Automated_Data_Processing_Yeriko_Vargas.ipynb',\n",
       " 'Receipts_Presentation.ipynb',\n",
       " 'VIDEO EDITING PRESENTATION.ipynb',\n",
       " 'DL_AI_Classify_Sentences_NLP_Presentation.ipynb',\n",
       " 'OCR_Presentation.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 # see current files in PRESENTATIONS\n",
    "folder_path = f'{direc}/JUPY_PRESENTATIONS'\n",
    "list_jupy = list_all_ipynb_files(folder_path)\n",
    "list_jupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e07d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IDENTIFY CLOSEST work with UPWORK Task DECRIPTION\n",
    "ref_notebook = list_jupy[2]   # -----------------------------------------USER INPUT\n",
    "source_file = f'JUPY_PRESENTATIONS/{ref_notebook}'\n",
    "\n",
    "# The name of the output file\n",
    "ref_new_name     = f'_a_copy_of_{ref_notebook}'\n",
    "ref_name_out_path = f'{desk_path}/upwork_now/'\n",
    "ref_name_out_pdf  = f'{ref_new_name[:-6]}.pdf'\n",
    "\n",
    "# FN fields\n",
    "notebook_file = source_file\n",
    "output_file = ref_new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfab881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Automated_Data_Processing_Yeriko_Vargas.ipynb :: has been copied here :: \n",
      " -> /Users/yerik/_apple_source/PY/JUPY/py0ds/CONTENT \n",
      "\n",
      " with this new name :: \n",
      " -> _a_copy_of_Automated_Data_Processing_Yeriko_Vargas.ipynb\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "exec(open(f\"{direc}/functions/_0ds_fn_2_copy_ipynb_file.py\", encoding=\"utf-8\").read())\n",
    "# \n",
    "print(f'\\n {ref_notebook} :: has been copied here :: \\n -> {direc} \\n\\n with this new name :: \\n -> {ref_new_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73389ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRANSFORM TO PDF\n",
    "from nbconvert import PDFExporter\n",
    "pdf_exporter = PDFExporter()\n",
    "pdf_data, resources = pdf_exporter.from_filename(f\"{ref_new_name}\")\n",
    "with open(f\"{ref_name_out_pdf}\", \"wb\") as f:\n",
    "    f.write(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd935be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Script to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89d6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET SUMMARY FROM the PDF\n",
    "how_many_words = 400 # -----------------------------------------USER INPUT\n",
    "import PyPDF2\n",
    "\n",
    "def read_ref_work_words(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    words = text.split()\n",
    "    first_words = ' '.join(words[:how_many_words])\n",
    "    return first_words\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_path = f'{ref_name_out_pdf}'\n",
    "str_ref_work = read_ref_work_words(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b90550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2938"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str_ref_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2cf73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------USR INPUT\n",
    "\n",
    "str_ref_job_desc = '''\n",
    "I am seeking a highly proficient and dedicated Virtual Assistant to join our team on a freelance basis via Upwork.com. As an Assistant, you will play a crucial role in providing phone call services, conducting extensive research, and performing various data-related tasks.\n",
    "\n",
    "Key Responsibilities:\n",
    "- Conducting phone call services with excellent English language skills, ensuring clarity and fluency in communication.\n",
    "- Performing comprehensive research on various topics, gathering relevant information efficiently.\n",
    "- Demonstrating proficiency in Microsoft Excel and Word to handle data management and documentation tasks.\n",
    "- Utilizing data mining techniques to extract valuable insights from various sources.\n",
    "- Merging data from multiple documents into a single cohesive document for analysis and reporting.\n",
    "\n",
    "Requirements:\n",
    "- Strong command over written and spoken English to ensure effective communication.\n",
    "- Proven experience in conducting phone call services with exceptional professionalism.\n",
    "- Proficiency in Microsoft Excel and Word, demonstrating the ability to effectively manage data and generate reports.\n",
    "- Excellent research skills, with the ability to gather accurate and relevant information from various sources.\n",
    "- Strong attention to detail and organizational skills to ensure accurate data merging and management.\n",
    "\n",
    "This is a temporary position with ongoing work, offering flexibility and the opportunity to work remotely. Upon reviewing applications, we will shortlist two candidates and discuss proposed timelines and milestones for completing assigned tasks. The candidate with the most feasible process and schedule will be selected.\n",
    "\n",
    "\n",
    "We look forward to discussing the opportunity.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39ad61",
   "metadata": {},
   "source": [
    "# CHAT GPT CHECK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae0ae106",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt_stat = f''' PLEASE GO AHEAD AND USE THIS REF WORK enclosed by \n",
    "                <<< ref_work >>> and match it with job description enclosed by\n",
    "                <(( job_desc ))> and give me a new angle to word my ref work to make it more relatable to job_desc\n",
    "                thus give me new intro starting by a new relatable title followed by Yeriko Vargas\n",
    "                :::\n",
    "                \n",
    "                <<< {str_ref_work} >>>\n",
    "                \n",
    "                :::\n",
    "                \n",
    "                <(( {str_ref_job_desc} ))>\n",
    "                \n",
    "                PLEASE GIVE ME THE DESCRIPTION and summary in a code like square so I can just do copy code as jupy markdown\n",
    "'''\n",
    "pyperclip.copy(chat_gpt_stat) # copies it to clipboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf1f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter notebook /Users/yerik/_apple_source/PY/JUPY/py0ds/CONTENT/_a_copy_of_Automated_Data_Processing_Yeriko_Vargas.ipynb ---> COPIED TO CLIPBOARD\n"
     ]
    }
   ],
   "source": [
    "# OPEN AND MODIFY\n",
    "to_modify = f'jupyter notebook {direc}/{ref_new_name}'\n",
    "print(f'{to_modify} ---> COPIED TO CLIPBOARD')\n",
    "\n",
    "pyperclip.copy(to_modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7ccd119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['osascript', '-e', 'tell application \"Termin...>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tab 1 of window id 24623\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get clipboard content\n",
    "clipboard_content = subprocess.getoutput('pbpaste')\n",
    "\n",
    "# Open a new Terminal window and run the clipboard content\n",
    "command_to_run = f'tell application \"Terminal\" to do script \"{clipboard_content}\"'\n",
    "subprocess.Popen([\"osascript\", \"-e\", command_to_run])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56d98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79c056f",
   "metadata": {},
   "source": [
    "# GO modify the new file and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2feb6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name_upwork = 'OCR_Based_Data_Management_Virtual_Assistant'  #---------------------------- USER INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6aee1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e677a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#application_string = input(' Paste the Application string ::: ') #---------------------------- USER INPUT\n",
    "\n",
    "application_string = '''\n",
    "# Proposal Application for Virtual Assistant Role\n",
    "\n",
    "## Title: Data-Driven Virtual Assistant Offering Comprehensive Research and Effective Data Management Solutions\n",
    "\n",
    "### Introduction: \n",
    "Hello,\n",
    "\n",
    "My name is Yeriko Vargas, and I am thrilled to apply for the Virtual Assistant role at your esteemed organization via Upwork.com. With a strong background in data science and Python coding, I bring a unique, data-driven approach to the key responsibilities outlined in your job description. \n",
    "\n",
    "### Skills Matching Job Requirements:\n",
    "\n",
    "#### Communication:\n",
    "- I am highly proficient in English, both written and spoken, ensuring clarity and fluency in phone call services.\n",
    "  \n",
    "#### Research:\n",
    "- Through Python, I've developed code for OCR, facilitating extensive research by converting text in high-efficiency image formats to machine-readable formats.\n",
    "  \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7958b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after modifying the file \n",
    "#\n",
    "# 1a # change it to a pdf\n",
    "#\n",
    "# # TRANSFORM TO PDF and send it to ---------- > upwork_now\n",
    "from nbconvert import PDFExporter\n",
    "pdf_exporter = PDFExporter()\n",
    "pdf_data, resources = pdf_exporter.from_filename(f\"{ref_new_name}\")\n",
    "with open(f\"{ref_name_out_path}/{new_name_upwork}.pdf\", \"wb\") as f:\n",
    "    f.write(pdf_data)\n",
    "#\n",
    "# 2a # save modified notebook to presentations \n",
    "#\n",
    "notebook_file = ref_new_name\n",
    "output_file = f'JUPY_PRESENTATIONS/{new_name_upwork}.ipynb'\n",
    "exec(open(f\"{direc}/functions/_0ds_fn_2_copy_ipynb_file.py\", encoding=\"utf-8\").read())\n",
    "print(f'\\n {notebook_file} :: ATTN !! after being modified has been copied here as :: \\n -> {output_file} ')\n",
    "#\n",
    "# 3a # save job descr and application with ref docs to folder f'{source}/z_ARCH_upwork_applications'\n",
    "#\n",
    "# application_string = input(' Paste the Application string ::: ')\n",
    "#\n",
    "save_proposal = f'''  {str_ref_job_desc}  \\n\\n {application_string }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "077f4fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2549"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save_proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "699c0b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OCR_Based_Data_Management_Virtual_Assistant'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine 2 pdfs to save archive proposal \n",
    "new_name_upwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39a4f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import PyPDF2\n",
    "\n",
    "# Create a new PDF with FPDF and your_string\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "your_string = save_proposal \n",
    "\n",
    "\n",
    "pdf.multi_cell(0, 10, txt=your_string)\n",
    "pdf_file_with_string = \"proposal.pdf\"\n",
    "pdf.output(pdf_file_with_string)\n",
    "\n",
    "# Open the existing PDF and the new PDF\n",
    "existing_pdf_file = open(f\"{ref_name_out_path}/{new_name_upwork}.pdf\", \"rb\")\n",
    "new_pdf_file = open(pdf_file_with_string, \"rb\")\n",
    "\n",
    "# Read the existing PDF and the new PDF using the new PdfReader class\n",
    "existing_pdf_reader = PyPDF2.PdfReader(existing_pdf_file)\n",
    "new_pdf_reader = PyPDF2.PdfReader(new_pdf_file)\n",
    "\n",
    "# Create a PdfWriter object to write the combined PDF\n",
    "pdf_writer = PyPDF2.PdfWriter()\n",
    "\n",
    "# Add the page with your_string\n",
    "pdf_writer.add_page(new_pdf_reader.pages[0])\n",
    "\n",
    "# Add all pages from the existing PDF\n",
    "for page_num in range(len(existing_pdf_reader.pages)):\n",
    "    page = existing_pdf_reader.pages[page_num]\n",
    "    pdf_writer.add_page(page)\n",
    "\n",
    "# Write the combined PDF to a file\n",
    "with open(f\"{direc}/z_ARCH_upwork_applications/{date_string}_UPWORK_PROPOSAL_and_REF_{new_name_upwork}.pdf\", \"wb\") as f_out:\n",
    "    pdf_writer.write(f_out)\n",
    "\n",
    "# Close the PDF files\n",
    "existing_pdf_file.close()\n",
    "new_pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ce3c351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_a_copy_of_Receipts_Presentation.ipynb'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60feaaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PLEASE GO AHEAD AND USE THIS REF WORK enclosed by \\n                <<< ref_work >>> and match it with job description enclosed by\\n                <(( job_desc ))> and give me a new angle to word my ref work to make it more relatable to job_desc\\n                thus give me new intro starting by a new relatable title followed by Yeriko Vargas\\n                :::\\n                \\n                <<< _a_copy_of_Receipts_Presentation September 14, 2023 1 OCR for RECEIPTS - by Y eriko V argas The code aims to solve the problem of converting multiple High-Eﬀiciency Image F ormat (HEIC) files to the more commonly used JPEG format in a batch process. HEIC is an image format used by Apple devices, but it’s not universally supported. This code snippet takes care of this issue by automatically scanning a directory for HEIC files, assigning unique identifiers to them, and then converting them to JPEGs. Library Imports: Libraries like os, pillow_heif, PIL, and cv2 are imported to facilitate the task. List HEIC Files: It scans the current directory and creates a list of all files with a .HEIC extension. Generate IDs for Filenames: The code generates unique identifiers for each file, appending a custom label to the original name minus the HEIC extension. Convert from HEIC to JPEG: It performs the conversion by reading the HEIC file, transforming it into a PIL Image object, and then saving it as a PNG temporarily . After that, it reads this PNG using cv2 and saves it as a JPEG. Execution: All of the steps are executed through a main function. The code accomplishes its tasks in an eﬀicient and organized manner, providing a quick solution to a common problem faced by users of Apple’s image format. [ ]: # +------------+ +----------------+ +----------------+ ␣ ↪+----------------+ # | List Files | ---> | Generate IDs | ---> | Convert to JPG | ---> | ␣ ↪Save as JPEG | # +------------+ +----------------+ +----------------+ ␣ ↪+----------------+ # Import required libraries import os import pillow_heif from PIL importImage import cv2 # Function to list all HEIC files in the current directory deflist_heic_files (): file_list =[] forfile inos.listdir(): iffile.endswith( \".HEIC\"): 1file_list .append(file) returnfile_list # Function to generate IDs for file names defgenerate_ids (file_list): id_str=[str(x) forxinlist(range(len(file_list)))] modified_names =[\"_receipt - \"+name.replace( \".HEIC\",\"\")forname in␣ ↪file_list] return[i+j fori, j inzip(id_str, modified_names)] # Function to convert HEIC files to JPG defconvert_heic_to_jpg (heic_files, output_names): fori inrange(len(heic_files)): heic_file =pillow_heif .read_heif(heic_files[i]) image=Image.frombytes(heic_file .mode, heic_file .size, heic_file .data,␣ ↪\"raw\") image.save(\"temp.png \",format(\"png\")) image=cv2.imread(\\'temp.png \\') cv2.imwrite( f\\'{output_names[i] }.jpg\\', image, [ int(cv2. ↪IMWRITE_JPEG_QUALITY), 100]) print(\"Successfully converted: \", output_names[i]) # Main function to execute the code defmain(): heic_files =list_heic_files() output_names =generate_ids(heic_files) convert_heic_to_jpg(heic_files, output_names) if__name__ ==\"__main__ \": main() 1.1 Optimizing Files for Best Results The code aims to improve the quality of a scanned document or an image. It focuses on six main stages: reading the image, resizing it, applying a Gaussian blur, enhancing its contrast, sharpening it, and finally binarizing it before saving the enhanced image. Problem It Solves Poor-quality scans can be diﬀicult to read, and re-scanning is not always an option. This code enhances scanned documents or grayscale images to make them more legible. How Does It Accomplish This? Read and Resize: The code begins by reading an input image and resizing it while maintaining its aspect ratio. Gaussian Blur: It applies a Gaussian blur to smoothen the image. Contrast Enhancement: The code enhances the contrast by equalizing the image histogram. Sharpening: The sharpness of the image is enhanced using unsharp masking. Binarization: The image is binarized, which means it is converted to a black and white format. Save Output: Finally , the enhanced image is saved. Diagram 2+---------------------+ | Read Image | +---------------------+ | V +---------------------+ | Resize Image | +---------------------+ | V +---------------------+ | Gaussian Blur | +---------------------+ | V +---------------------+ | Enhance Contrast | +---------------------+ | V +---------------------+ | Sharpening | +---------------------+ | V +---------------------+ | Binarization | +---------------------+ | V +---------------------+ | Save Output | +---------------------+ [ ]: #Import required libraries import numpy as np import os from PIL importImage, ImageFilter defenhance_document (input_file, output_file, max_dimension =800,␣ ↪blur_radius =10, contrast_factor =2.0, sharpen_factor =2.0,␣ ↪threshold_block_size =11, threshold_c =5): \"\"\" Enhances the quality of a scanned document. Parameters: - input_file (str): The name of the input image file 3- output_file (str): The name of the output image file - max_dimension (int): The maximum dimension for resizing - blur_radius (float): Radius for Gaussian blur - contrast_factor (float): Factor to enhance contrast - sharpen_factor (float): Factor to enhance sharpness - threshold_block_size (int): Block size for adaptive thresholding - threshold_c (float): Constant for adaptive thresholding \"\"\" # Step 1: Read and Resize the Image input_path =os.path.join(os.getcwd(), input_file) img=Image.open(input_path) .convert( \"L\") width, height =img.size new_width, new_height =(max_dimension, int(max_dimension *height/␣ ↪width)) ifwidth>height else(int(max_dimension *width/height), ␣ ↪max_dimension) img_resized =img.resize((new_width, new_height), resample =Image.BICUBIC) # Step 2: Apply Gaussian Blur img_blur =img_resized .filter(ImageFilter .GaussianBlur(radius =blur_radius)) # Step 3: Enhance Contrast img_eq=Image.fromarray(np .uint8(np .array(img_blur) *255)) img_eq=img_eq.point( lambdax: x*contrast_factor) # Step 4: Sharpen Image img_sharp =img_blur .filter(ImageFilter .UnsharpMask(radius =2,␣ ↪percent=sharpen_factor)) # Step 5: Binarize Image img_thresh =img_sharp .point( lambdax:255 ifx>np.mean(img_sharp) else0) img_thresh =img_thresh .filter(ImageFilter .MedianFilter(size =3)) img_thresh =img_thresh .point( lambdax:255 ifx>img_thresh .point( lambda␣ ↪y: np.mean(y) +threshold_c) else0) # Step 6: Save the Output Image output_path =os.path.join(os.getcwd(), output_file) img_thresh .save(output_path) # Example usage enhance_document( \"image0.jpeg \",\"image_out.jpeg \") 2 Read and send RECEIPTS fields to a data frame Image Reading: The function ocr_to_table reads image files from a given directory path. OCR Processing: pytesseract performs OCR on these images, converting the image text into machine- readable strings. Information Extraction: The script then employs Regular Expressions to extract specific details like the date, amount, and card information. 4Diagram +---------------------+ | Image Reading | +---------------------+ | V +---------------------+ | OCR Processing | +---------------------+ | V +---------------------+ | Information | | Extraction | +---------------------+ [ ]: import re from datetime importdatetime import pytesseract import pandas as pd defocr_to_table (path_source, pic_ids): \"\"\" Function to OCR images from a directory and transform the fields of a ␣ ↪receipt into a DataFrame. \"\"\" data=[] fori inpic_ids: image_path =f\\'{path_source }/PY/MAIN/KIMVA/Receipts/ {i}\\' text=pytesseract .image_to_string(image_path, lang =\\'eng\\') # Date Extraction date=re.search(r\\'\\\\d{4}/\\\\d{2}/\\\\d{2}\\', text) date=datetime .strptime(date .group(), \\'%Y/%m/ %d\\').date() ifdate else␣ ↪None # Amount Extraction amount=re.search(r\"\\\\$\\\\d+\\\\.\\\\d{2}\", text) amount=amount.group() ifamount else None # Card Extraction card=re.search(r\"Card\\\\s*:\\\\s*\\\\d{4}\", text) card=card.group() ifcard else None # Adding to the data list 5data.append([i, date, amount, card]) # Creating DataFrame df=pd.DataFrame(data, columns =[\\'pic_id_name \\',\\'Transaction Date \\',\\'Total␣ ↪Amount\\',\\'Card Info \\']) returndf [ ]: #end 6 >>>\\n                \\n                :::\\n                \\n                <(( \\nI am seeking a highly proficient and dedicated Virtual Assistant to join our team on a freelance basis via Upwork.com. As an Assistant, you will play a crucial role in providing phone call services, conducting extensive research, and performing various data-related tasks.\\n\\nKey Responsibilities:\\n- Conducting phone call services with excellent English language skills, ensuring clarity and fluency in communication.\\n- Performing comprehensive research on various topics, gathering relevant information efficiently.\\n- Demonstrating proficiency in Microsoft Excel and Word to handle data management and documentation tasks.\\n- Utilizing data mining techniques to extract valuable insights from various sources.\\n- Merging data from multiple documents into a single cohesive document for analysis and reporting.\\n\\nRequirements:\\n- Strong command over written and spoken English to ensure effective communication.\\n- Proven experience in conducting phone call services with exceptional professionalism.\\n- Proficiency in Microsoft Excel and Word, demonstrating the ability to effectively manage data and generate reports.\\n- Excellent research skills, with the ability to gather accurate and relevant information from various sources.\\n- Strong attention to detail and organizational skills to ensure accurate data merging and management.\\n\\nThis is a temporary position with ongoing work, offering flexibility and the opportunity to work remotely. Upon reviewing applications, we will shortlist two candidates and discuss proposed timelines and milestones for completing assigned tasks. The candidate with the most feasible process and schedule will be selected.\\n\\n\\nWe look forward to discussing the opportunity.\\n\\n ))>\\n                \\n                PLEASE GIVE ME THE DESCRIPTION and summary in a code like square so I can just do copy code as jupy markdown\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0680f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35231cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3ce6726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time as string: 2023_09_14\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time\n",
    "now = datetime.now(); date_string = now.strftime(\"%Y_%m_%d\")#date_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70bd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ac1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20963954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e6572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a263767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d375d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functions/_0ds_statements_1_upwork_application_statement.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/_0ds_statements_1_upwork_application_statement.py\n",
    "\n",
    "custom_instructions_upwrk_about_me = ''' \n",
    "Work exp:-------------------->(>3.5 years)--------------------\n",
    "\n",
    "By now I have over 3 and a half years of professional experience in the field of ML DL acting as a data scientist.\n",
    "\n",
    "Python R:--------------> 8 years\n",
    "Stats and Math:--------> 10 years \n",
    "SQL and Hadoop :-------> 3 years\n",
    "Tableau:---------------> 3 years\n",
    "\n",
    "FORD |                                                                                                                          \n",
    "Lead Data Scientist (contract)\n",
    "\n",
    "•\tManaged, manipulated, and analyzed big data from CVDOS data base which captured more data in the last 2 years compared to previous 50 years count using python to fulfill internal client data requirements\n",
    "•\tActed as an expert transforming, merging, and visualizing data sets using python and internal AWS Sage maker services with Jupiter notebooks to send statistics reports and updates daily\n",
    "\n",
    "Soothsayer Analytics | Livonia, MI                                                                                                   Machine Learning Researcher (full time)\n",
    "\n",
    "Fiat Chrysler Automotive | Auburn Hills, MI                                                                                       \n",
    "Deep Learning Specialist (contract)\n",
    "\n",
    "Master of Science in Applied Statistics | Oakland University, Auburn Hills, MI                                                 May 2020\n",
    "Bachelor of Science in Applied Statistics | Oakland University, Auburn Hills, MI                                              Apr 2017\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "custom_instructions_upwrk_respond ='''\n",
    "\n",
    "\n",
    "I need you to write the proposals as if you were looking at the mean of responses, and give me always the tone of the writing. like retrospect and think, the pool of answers at least 51% of the recruiters will say yes. I will just add, not too funny, but have some humor, and not too serious, seem excited and surprised, but don't come across arrogant or as \"trying too hard\"\n",
    "\n",
    "Be concise on the intro and the small talk, but when there is opportunity to connect to what the job seeker looks for, then go more on that and how my work experience relates.\n",
    "\n",
    "do Always give me a plan like milestones of project with a decent timeline to finish them. always have a signature at the bottom\n",
    "'''\n",
    "\n",
    "chat_gpt_stat_prop =f''' after (1) learning about the job opportunity I am deiciing to apply and (2) knowing the work \n",
    "                reference we summarized together, now build a formal proposal with the following work experience\n",
    "                of mine, as well as my education, which I will ask you to show case in a organized way.\\n\\n\n",
    "                \n",
    "                <<< start: in between these symbols custom_instructions_upwrk_about_me >>>\n",
    "                \n",
    "                <<< {custom_instructions_upwrk_about_me} >>> \\n\\n\n",
    "                \n",
    "                \n",
    "                *** in between these symbols custom_instructions_upwrk_respond ***\n",
    "                \n",
    "                *** {custom_instructions_upwrk_respond} *** \\n\\n\n",
    "                \n",
    "                Lastly (1) Ask me if I want to add milestones? and \n",
    "                (2) Hey Yeriko are there any custom questions that the proposal is showing  \n",
    "                Just if I answer you those questions you can continue. Go ->\n",
    "                \n",
    "                    \n",
    "\n",
    "'''\n",
    "pyperclip.copy(chat_gpt_stat_prop)\n",
    "print(' print(chat_gpt_stat_prop) to see what is copied to CLIPBOARD - > go PASTE TO CHAT GPT to get PROPOSAL !!! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa9c72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing functions/_0ds_fn_5_get_title_of_new_work_ref.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/_0ds_fn_5_get_title_of_new_work_ref.py\n",
    "\n",
    "import json\n",
    "\n",
    "def get_first_markdown_title(notebook_path):\n",
    "    with open(notebook_path, 'r') as f:\n",
    "        notebook_data = json.load(f)\n",
    "\n",
    "    for cell in notebook_data['cells']:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            source = cell['source']\n",
    "            for line in source:\n",
    "                if line.startswith('#'):\n",
    "                    return line.strip()[2:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a52499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing functions/_0ds_fn_4_get_description_to_modify_in_ref_work.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/_0ds_fn_4_get_description_to_modify_in_ref_work.py \n",
    "import PyPDF2\n",
    "\n",
    "def read_ref_work_words(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "    words = text.split()\n",
    "    first_words = ' '.join(words[:how_many_words])\n",
    "    return first_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df98db5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing functions/_0ds_fn_3_NLP_match_job_desc_and_ref_work.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/_0ds_fn_3_NLP_match_job_desc_and_ref_work.py \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def find_best_match(job_desc, document_titles):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # Combine the job description with the list of document titles\n",
    "    all_docs = [job_desc] + document_titles\n",
    "    \n",
    "    # Generate the TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
    "    \n",
    "    # Compute the cosine similarity between the job description and each document title\n",
    "    cos_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "    \n",
    "    # Get the index of the most similar document\n",
    "    most_similar_idx = np.argmax(cos_similarities)\n",
    "    \n",
    "    # Return the title of the most similar document\n",
    "    return document_titles[most_similar_idx]\n",
    "\n",
    "# Your list of document titles\n",
    "document_titles = list_jupy\n",
    "# Sample job description\n",
    "str_ref_job_desc = str_ref_job_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e41da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functions/_0ds_fn_2_copy_ipynb_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/_0ds_fn_2_copy_ipynb_file.py \n",
    "\n",
    "from nbconvert import NotebookExporter\n",
    "import nbformat\n",
    "\n",
    "# Initialize NotebookExporter\n",
    "exporter = NotebookExporter()\n",
    "\n",
    "# Read the notebook using nbformat\n",
    "notebook_content = nbformat.read(notebook_file, as_version=4)\n",
    "\n",
    "# Export the notebook into a different format\n",
    "exported_content, resources = exporter.from_notebook_node(notebook_content)\n",
    "\n",
    "# Write the exported content to a file\n",
    "with open(output_file, 'wb') as f:\n",
    "    f.write(exported_content.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78cafa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functions/_0ds_fn_1_get_ipynb_file_names.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/_0ds_fn_1_get_ipynb_file_names.py \n",
    "import os\n",
    "\n",
    "def list_all_ipynb_files(folder_path):\n",
    "    try:\n",
    "        all_files = os.listdir(folder_path)\n",
    "    except FileNotFoundError:\n",
    "        return \"Folder not found.\"\n",
    "    \n",
    "    ipynb_files = [f for f in all_files if f.endswith('.ipynb')]\n",
    "    \n",
    "    return ipynb_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a5bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2d11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40ebd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f14f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197640c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aefe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d837f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b72c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c45fd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7974e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743aaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
